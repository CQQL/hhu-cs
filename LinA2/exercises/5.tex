\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\begin{document}

\section*{Übung 1}

\begin{proof}
Um zu zeigen, dass $\mu_{f}$ das Minimalpolynom ist, müssen wir zeigen, dass $\mu_{f}(f) = 0$ und $\mu_{f} | P$ für alle $P \in K[X]$ mit $P(f) = 0$.

Wir wissen, dass $V$ die direkte Summe seiner $r$ Haupträume ist, wobei $H(f, \lambda_{i}) = E_{m_{i}}(f, \lambda_{i})$ ist.
Dann können wir jedes $v \in V$ als $v_{1} + \dots + v_{r}$ darstellen, wobei $v_{i} \in H(f, \lambda_{i})$.
Wir wissen außerdem, dass die Haupträume $f$-invariant und somit auch $f - \lambda_{i}id$-invariant.
Also ist $(f - \lambda_{k}id)(v_{j}) \in H(f, \lambda_{j})$ für alle $k \in [1, r]$.
Deshalb ist $(f - \lambda_{i}id)^{m_{i}}(v_{1} + \dots + v_{r}) = (w_{1} + \dots + w_{i - 1} + 0 + w_{i + 1} + \dots + w_{r})$, wobei $w_{k} \in H(f, \lambda_{k})$.
Insgesamt ist also $\mu_{f}(f)(v) = 0 \quad \forall v \in V$ und $\mu_{f}(f) = 0$.

Sei $P \in K[X]$ mit $P(f) = 0$.
Wenn $P = 0$, gilt $\mu_{f} | P$, weil das $0$-Polynom von jedem Polynom geteilt wird.
Sei also $P \ne 0$.
Angenommen $P$ enthalte den Linearfaktor $(f - \lambda_{i})^{m_{i}}$ nicht.
Dann ist $P(f)(v) \ne 0$ für $v \in E_{m_{i}}(f, \lambda_{i}) \setminus E_{m_{i} - 1}(f, \lambda_{i})$ im Widerspruch zur Voraussetzung.
Also enthält $P$ alle diese Linearfaktoren und $\mu_{f} | P$.
\end{proof}

\section*{Übung 2}

\subsection*{Teil 1}

Jeder Unterraum von $V$, der die Bedingungen erfüllt, muss mindestens $v$ enthalten und $u$ mit $u = f^{k}(v)$.
Es muss also ein Unterraum von $U$ sein.
Sei $W$ ein solcher Unterraum mit $W \subsetneq U$.
Dann gibt es ein minimales $u \in \mathbb{N}$, sodass $f^{u}(v) \notin W$.
Wenn $u = 0$, ist $f^{u}(v) = v \notin W$ und $W$ erfüllt die Bedingung nicht mehr.
Wenn $u > 0$, ist $f^{u - 1}(v) \in W$, aber $f(f^{u - i}(v)) \notin W$ und $W$ wäre nicht $f$-invariant.
Damit $v \in W$ und $W$ $f$-invariant sein kann, darf also kein solches $u$ existieren und es muss $W = U$ sein.

\subsection*{Teil 2}

\subsection*{Teil 3}

\begin{proof}
Sei $v = (v_{1}, v_{2}, v_{3}) \in \mathbb{R}^{3}$.
\begin{equation}
f(v) = Av = \begin{pmatrix}
v_{1} + v_{2} - v_{3}\\
-2v_{1} - 2v_{2} + 2v_{3}\\
v_{1} + v_{2} - v_{3}
\end{pmatrix}
= \begin{pmatrix}
z\\-2z\\z
\end{pmatrix}
\end{equation}
\begin{equation}
f(\begin{pmatrix}
z\\-2z\\z
\end{pmatrix}) = A\begin{pmatrix}
z\\-2z\\z
\end{pmatrix}
= \begin{pmatrix}
-2z\\4z\\-2z
\end{pmatrix}
= -2 \cdot \begin{pmatrix}
z\\-2z\\z
\end{pmatrix}
\end{equation}
Also sind für jedes $v \in \mathbb{R}^{3}$ $f^{j}(v)$ und $f^{k}(v)$ für alle $j, k \ge 1$ linear abhängig und $(v, f^{k}(v))$ für ein $k \ge 1$ ist die größtmögliche Basis von $U_{v}$.
Diese hat aber nur 2 Elemente, sodass $U_{v} \ne \mathbb{R}^{3}$ sein muss.
\end{proof}

\section*{Übung 3}

\subsection*{Teil 1}

\begin{proof}
Ich zeige es per Induktion über $k$.

Angenommen, $\sigma(1) \ne 1$.
Dann muss es $j \in [2, n]$ geben mit $\sigma(j) = 1$.
Aber dann muss $\sigma(j - 1) \ne 1$ sein, also $\sigma(j - 1) > 1 = \sigma(j)$ im Widerspruch zur Annahme.

Angenommen, $1 < k < n - 1$ und $\sigma(j) = j$ für alle $j \in [1, k - 1]$.
Weiterhin angenommen $\sigma(k) \ne k$, also $\sigma(k) > k$.
Dann muss es $j \in [k + 1, n]$ geben mit $\sigma(j) = k$.
Aber $\sigma(j - 1)$ muss $> k$ sein, also $\sigma(j - 1) > k = \sigma(j)$ im Widerspruch zur Annahme.

Angenommen, $k = n - 1$ und $\sigma(j) = j$ für alle $j \in 1, k - 1]$.
Wenn $\sigma(n - 1) \ne n - 1$, muss $\sigma(n - 1) = n - 2$ und $\sigma(n - 2) = n - 1$ sein im Widerspruch zur Annahme.

Angenommen, $k = n$ und $\sigma(j) = j$ für alle $j \in 1, k - 1]$.
Dann muss $\sigma(n) = n$ sein.

Insgesamt muss $\sigma(j) = j \quad \forall j \in [1, n]$ und $\sigma = id$ sein.
\end{proof}

\subsection*{Teil 2}

\begin{proof}
Sei $(k, l) \in s_{i}(I(\sigma))$.
\begin{align*}
(k, l) = (i + 1, i) \Rightarrow (k, l) \in I(\sigma s_{1}) \cup \{(i + 1, i)\}\\
(k, l) = (k, i) \Rightarrow (k, i + 1) \in I(\sigma) \Rightarrow (k, i) \in I(\sigma s_{i})\\
(k, l) = (k, i + 1) \Rightarrow (k, i) \in I(\sigma) \Rightarrow (k, i + 1) \in I(\sigma s_{i})\\
(k, l) = (i, l) \Rightarrow (i + 1, l) \in I(\sigma) \Rightarrow (i, l) \in I(\sigma s_{i})\\
(k, l) = (i + 1, l) \Rightarrow (i, l) \in I(\sigma) \Rightarrow (i + 1, l) \in I(\sigma s_{i})\\
k \ne i \land k \ne i + 1 \land l \ne i \land l \ne i + 1 \Rightarrow (k, l) \in I(\sigma s_{i})
\end{align*}

Sei $(k, l) \in I(\sigma s_{i})$.
\begin{align*}
(k, l) = (i + 1, i) \Rightarrow (i, i + 1) \in I(\sigma) \Rightarrow (i + 1, i) \in s_{i}I(\sigma)\\
(k, l) = (k, i) \Rightarrow (k, i + 1) \in I(\sigma) \Rightarrow (k, i) \in s_{i}I(\sigma)\\
(k, l) = (k, i + 1) \Rightarrow (k, i) \in I(\sigma) \Rightarrow (k, i + 1) \in s_{i}I(\sigma)\\
(k, l) = (i, l) \Rightarrow (i + 1, l) \in I(\sigma) \Rightarrow (i, l) \in s_{i}I(\sigma)\\
(k, l) = (i + 1, l) \Rightarrow (i, l) \in I(\sigma) \Rightarrow (i + 1, l) \in s_{i}I(\sigma)\\
k \ne i \land k \ne i + 1 \land l \ne i \land l \ne i + 1 \Rightarrow (k, l) \in s_{i}I(\sigma)
\end{align*}

Da $(i, i + 1) \in I(\sigma)$, ist $(i + 1, i) \notin I(\sigma s_{i})$, weil $\sigma s_{i}(i) < \sigma s_{i}(i + 1)$.
Also ist $I(\sigma s_{i}) \cap \{(i + 1, i)\} = 0$ und es gilt
\begin{equation}
|s_{i}I(\sigma)| = |I(\sigma)| = \ell(\sigma) = |I(\sigma s_{i}) \cup \{(i + 1, i)\}| = |I(\sigma s_{i})| + 1 = \ell(\sigma s_{i}) + 1
\end{equation}
\end{proof}

\subsection*{Teil 3}

\begin{proof}
Sei $\ell(\sigma) = 0$.
Es ist $\varepsilon(\sigma) = \det(I_{n}) = 1 = (-1)^{0} = (-1)^{\ell(\sigma)}$.

Angenommen $\varepsilon(\sigma s_{i}) = (-1)^{\ell(\sigma s_{i})}$, wobei $s_{i}$ wie in Teil 2 gewählt wird, sodass $\ell(\sigma) = \ell(\sigma s_{i}) + 1$.
Dann ist $\sigma = \sigma s_{i} s_{i}$ und
\begin{equation}
\varepsilon(\sigma) = \det(P_{\sigma s_{i} s_{i}}) = \det(E_{i,i + 1}P_{\sigma s_{i}}) = \det(E_{i,i + 1}) \cdot \det(P_{\sigma s_{i}}) = (-1) \cdot (-1)^{\ell(\sigma s_{i})} = (-1)^{\ell(\sigma)}
\end{equation}
\end{proof}

\section*{Übung 4}

\subsection*{Teil 1}

\begin{equation}
\sigma = s_{3}s_{2}s_{1}s_{3}s_{2}s_{4}s_{3}
\end{equation}
\begin{equation}
\ell(\sigma) = 7
\end{equation}
\begin{equation}
\varepsilon(\sigma) = (-1)^{7} = -1
\end{equation}

\subsection*{Teil 2}

\begin{align*}
\varepsilon(\sigma_{1}) = -1\\
\varepsilon(\sigma_{2}) = 1\\
\varepsilon(\sigma_{3}) = -1\\
\varepsilon(\sigma_{4}) = 1
\end{align*}

\begin{equation}
\varepsilon(\sigma) = 1 \cdot 1 \cdot (-1) \cdot 1 \cdot 1 \cdot 1 \cdot (-1) \cdot 1 \cdot (-1) \cdot 1 \cdot 1 \cdot 1 \cdot (-1) \cdot 1 = 1
\end{equation}

\section*{Übung 5}

\subsection*{Teil 1}

\begin{proof}
Sei $\xi \in K^{(U \times V)}$ und $(w, z) \in U \times V$.
Dann ist
\begin{equation}
\xi(w, z) = \sum_{(u, v) \in U \times V} \xi(u, v) \cdot \varphi_{(u, v)}(w, z) = \xi(w, z) \cdot \varphi_{(w, z)}(w, z) = \xi(w, z)
\end{equation}
und $(\varphi_{(u, v)})_{(u, v) \in U \times V}$ ist ein EZS.

Für die lineare Unabhängigkeit betrachten wir
\begin{align}
& \sum_{(u, v) \in U \times V} \lambda_{u, v} \cdot \varphi_{(u, v)} = 0\\
\Leftrightarrow & \sum_{(u, v) \in U \times V} \lambda_{u, v} \cdot \varphi_{(u, v)}(w, z) = 0 \quad \forall (w, z) \in U \times V\\
\Leftrightarrow & \lambda_{w, z} \cdot \varphi_{(w, z)}(w, z) = 0 \quad \forall (w, z) \in U \times V\\
\Leftrightarrow & \lambda_{w, z} = 0 \quad \forall (w, z) \in U \times V\\
\end{align}
Also ist das System auch linear unabhängig und somit eine Basis.
\end{proof}

\subsection*{Teil 2}

\end{document}