\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\title{LinA2, Übungsblatt 1}
\author{Marten Lienen (2126759)}

\begin{document}

\maketitle

\section*{Übung 1}

\subsection*{1}

\subsubsection*{1.a}

\begin{equation}
ZSF(A_{1}) = 
\begin{pmatrix}
1 & -1 & 0\\
0 & 0 & 1\\
0 & 0 & 0
\end{pmatrix}
\end{equation}
\begin{equation}
Rg(A_{1}) = 2
\end{equation}

\subsubsection*{1.b}

\begin{equation}
\chi_{A_{1}} = X(X-2)(X-3)
\end{equation}

\subsubsection*{1.c}

\begin{equation}
\mu_{A_{1}} = X(X-2)(X-3)
\end{equation}

\subsubsection*{1.d}

$A_{1}$ ist diagonalisierbar, weil $\mu_{A_{1}}$ vollständig in Linearfaktoren einfacher Vielfachheit zerfällt.

\subsubsection*{1.e}

\begin{equation}
\langle
\begin{pmatrix}
1\\1\\0
\end{pmatrix},
\begin{pmatrix}
0\\0\\1
\end{pmatrix},
\begin{pmatrix}
1\\-2\\0
\end{pmatrix}
\rangle
\end{equation}

\subsubsection*{2.a}

\begin{equation}
ZFS(A_{2}) = I_{3}
\end{equation}
\begin{equation}
Rg(A_{2}) = 3
\end{equation}

\subsubsection*{2.b}

\begin{equation}
\chi_{A_{2}} = X(X-2)(X-3)
\end{equation}

\subsubsection*{2.c}

\begin{equation}
\mu_{A_{2}} = X(X-2)(X-3)
\end{equation}

\subsubsection*{2.d}

$A_{2}$ ist diagonalisierbar, weil $\mu_{A_{2}}$ vollständig in Linearfaktoren einfacher Vielfachheit zerfällt.

\subsubsection*{2.e}

\begin{equation}
\langle
\begin{pmatrix}
-2\\0\\1
\end{pmatrix},
\begin{pmatrix}
-1\\0\\1
\end{pmatrix},
\begin{pmatrix}
0\\1\\0
\end{pmatrix}
\rangle
\end{equation}

\subsubsection*{3.a}

\begin{equation}
ZSF(A_{3}) = 
\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 1\\
0 & 0 & 0
\end{pmatrix}
\end{equation}
\begin{equation}
Rg(A_{3}) = 2
\end{equation}

\subsubsection*{3.b}

\begin{equation}
\chi_{A_{3}} = X(X-2)(X-3)
\end{equation}

\subsubsection*{3.c}

\begin{equation}
\mu_{A_{3}} = X(X-2)(X-3)
\end{equation}

\subsubsection*{3.d}

$A_{3}$ ist diagonalisierbar, weil $\mu_{A_{3}}$ vollständig in Linearfaktoren einfacher Vielfachheit zerfällt.

\subsubsection*{3.e}

\begin{equation}
\langle
\begin{pmatrix}
1\\0\\0
\end{pmatrix},
\begin{pmatrix}
0\\-1\\1
\end{pmatrix},
\begin{pmatrix}
3\\1\\0
\end{pmatrix}
\rangle
\end{equation}

\subsubsection*{4.a}

\begin{equation}
ZSF(A_{4}) = 
\begin{pmatrix}
1 & 1 & -1\\
0 & 0 & 0\\
0 & 0 & 0
\end{pmatrix}
\end{equation}
\begin{equation}
Rg(A_{4}) = 1
\end{equation}

\subsubsection*{4.b}

\begin{equation}
\chi_{A_{4}} = x^{3}
\end{equation}

\subsubsection*{4.c}

\begin{equation}
\mu_{A_{4}} = x^{2}
\end{equation}

\subsubsection*{4.d}

$A_{4}$ ist nicht diagonalisierbar, weil $\mu_{A_{4}}$ eine Nullstelle mit Vielfachheit $2$ besitzt.

\subsubsection*{4.e}

Existiert nicht.

\subsubsection*{5.a}

\begin{equation}
ZSF(A_{5}) = 
\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 1\\
0 & 0 & 0
\end{pmatrix}
\end{equation}
\begin{equation}
Rg(A_{5}) = 2
\end{equation}

\subsubsection*{5.b}

\begin{equation}
\chi_{A_{5}} = x^{3}
\end{equation}

\subsubsection*{5.c}

\begin{equation}
\mu_{A_{5}} = x^{3}
\end{equation}

\subsubsection*{5.d}

$A_{5}$ ist nicht diagonalisierbar, weil $\mu_{A_{5}}$ eine Nullstelle mit Vielfachheit $3$ besitzt.

\subsubsection*{5.e}

Existiert nicht.

\subsubsection*{6.a}

\begin{equation}
ZSF(A_{6}) = 
\begin{pmatrix}
0 & 1 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{pmatrix}
\end{equation}
\begin{equation}
Rg(A_{6}) = 1
\end{equation}

\subsubsection*{6.b}

\begin{equation}
\chi_{A_{6}} = x^{3}
\end{equation}

\subsubsection*{6.c}

\begin{equation}
\mu_{A_{6}} = x^{2}
\end{equation}

\subsubsection*{6.d}

$A_{6}$ ist nicht diagonalisierbar, weil $\mu_{A_{6}}$ eine Nullstelle mit Vielfachheit $2$ besitzt.

\subsubsection*{6.e}

Existiert nicht.

\subsubsection*{7.a}

\begin{equation}
ZSF(A_{7}) = 
\begin{pmatrix}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{pmatrix}
\end{equation}
\begin{equation}
Rg(A_{7}) = 0
\end{equation}

\subsubsection*{7.b}

\begin{equation}
\chi_{A_{7}} = x^{3}
\end{equation}

\subsubsection*{7.c}

\begin{equation}
\mu_{A_{7}} = x
\end{equation}

\subsubsection*{7.d}

$A_{7}$ ist diagonalisierbar, weil es bereits eine Diagonalmatrix ist.

\subsubsection*{7.e}

Da $A_{7}$ die Nullmatrix ist, bildet sie alle Werte auf $0$ ab und es gilt $E(A_{7}, 0) = K^{3}$.
Daher wähle ich die kanonische Basis als Basis aus Eigenvektoren.

\subsection*{2}

\begin{equation}
A_{1} \approx A_{3}
\end{equation}
\begin{equation}
A_{4} \approx A_{6}
\end{equation}

\section*{Übung 2}

\subsection*{1}

\begin{proof}
Seien $P, Q \in V$ und $a, b \in K$.
\begin{align}
f(aP + bQ)(X) & = (aP + bQ)(X) + (1 - X)(aP + bQ)'(X)\\
& = (aP + bQ)(X) + (1 - X)(aP' + bQ')(X)\\
& = aP(X) + bQ(X) + (1 - X) \left( aP'(X) + bQ'(X) \right)\\
& = a \left( P(X) + (1 - X)P'(X) \right) + b \left( Q(X) + (1 - X)bQ'(X) \right)\\
& = a \cdot f(P)(X) + b \cdot f(Q)(X)
\end{align}
\end{proof}

\subsection*{2}

\begin{proof}
\begin{align*}
f \left( (X - 1)^{j} \right) & = (X - 1)^{j} + (1 - X) \cdot j \cdot (X - 1)^{j - 1}\\
& = (X - 1)^{j} - (X - 1) \cdot j \cdot (X - 1)^{j - 1}\\
& = (X - 1)^{j} - j \cdot (X - 1)^{j}\\
& = (1 - j) \cdot (X - 1)^{j}\\
\end{align*}
Es gibt also Eigenvektoren $(X - 1)^{j}$ in $V$ für jedes $j \in [0, n]$ mit Eigenwert $1 - j$.
Da ihre Eigenwerte alle paarweise verschieden sind, ist $f$ diagonalisierbar.
Diese $n + 1$ genannten Eigenvektoren, bilden auch gleich meine Basis aus Eigenvektoren.

Ich werde zeigen, dass sie linear unabhängig und ein EZS sind.
Dazu fasse ich die Polynome als Vektoren im $K^{n + 1}$ auf.

Seien $\lambda_{k}, k \in [0, n]$ Skalare und $v_{j} = (X - 1)^{j}, j \in [0, n]$ die Eigenvektoren.
\begin{equation}
\sum_{k = 0}^{n} \lambda_{k} \cdot v_{k} = 0
\end{equation}
Da jeder Eigenvektor $v_{j}$ ein normiertes Polynom von Grad $j$ ist, bleibt in der untersten Zeile dieser Gleichung von Vektoren nur folgendes übrig
\begin{equation}
\lambda_{n} \cdot v_{n} = \lambda_{n} = 0
\end{equation}
Induktiv schließt man, dass alle $\lambda_{k}$ $0$ sind, weil für jedes $k \in [0, n]$ sind nur die Vektoren $v_{j}, j \in [k, n]$ nicht $0$ an der $k$-ten Stelle, aber die Skalare $\lambda_{q}, q \in [k + 1, n]$ sind alle $0$.
Das System ist also linear unabhängig.

Sei $q \in V$ und $q_{k}$ sein $k$-ter Koeffizient.
Per Induktion über den Index des Koeffizienten $k$ zeige ich, dass man jeden Koeffizienten von $q$ als Linearkombination der Eigenvektoren darstellen kann.

Sei $k = n$.
Da es nur einen Vektor $v_{n}$ gibt, der an der $n$-ten Stelle nicht $0$ ist, wählen wir als $\lambda_{n}$ einfach $q_{n}$.

Sei $k < n$ und wir nehmen an, dass wir bereits eine Konfiguration $\lambda_{j}, j \in [k + 1, n]$ gefunden haben, sodass die Koeffizienten $k + 1$ bis $n$ von $q$ korrekt als Linearkombination der Eigenvektoren dargestellt werden.
Wir wählen
\begin{equation}
\lambda_{k} = -\left( \sum_{j = k + 1}^{n} \lambda_{j} \cdot v_{j_{k}} \right) + q_{k}
\end{equation}
Damit ergibt sich für den $k$-ten Koeffizienten der Wert
\begin{align}
\sum_{j = 0}^{n} \lambda_{j} \cdot v_{j_{k}} & = \sum_{j = k}^{n} \lambda_{j} \cdot v_{j_{k}}\\
& = \left( \sum_{j = k + 1}^{n} \lambda_{j} \cdot v_{j_{k}} \right) + \lambda_{k} \cdot v_{k_{k}}\\
& = \left( \sum_{j = k + 1}^{n} \lambda_{j} \cdot v_{j_{k}} \right) + \lambda_{k}\\
& = \left( \sum_{j = k + 1}^{n} \lambda_{j} \cdot v_{j_{k}} \right) + \left( -\left( \sum_{j = k + 1}^{n} \lambda_{j} \cdot v_{j_{k}} \right) + q_{k} \right)\\
& = q_{k}
\end{align}
Wenn wir bei $k = 0$ angekommen sind, haben wir eine vollständige Konfiguration, sodass
\begin{equation}
\sum_{j = 0}^{n} \lambda_{j} \cdot v_{j} = q
\end{equation}

Die Eigenvektoren bilden also ein EZS und sind somit eine Basis von $V$.
\end{proof}

\section*{Übung 3}

\subsection*{1}

Nein, sie sind nicht ähnlich, weil ihre Determinanten nicht gleich sind.
\begin{equation}
det(A) = -2
\end{equation}
\begin{equation}
det(B) = -1
\end{equation}

\subsection*{2}

Nein, sie sind nicht ähnlich, weil ihre Determinanten nicht gleich sind.
\begin{equation}
det(A) = 0
\end{equation}
\begin{equation}
det(B) = 1
\end{equation}

\end{document}